#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
profile_svd_kv_flash.py — SVD on weights + dense KV cache (FlashAttention), clean prefill/decode split, no big logits.

What this script does
---------------------
- Replaces GPT-2 blocks with an SVD-factorized block (Q/K/V, out-proj, MLP) but
  **keeps the KV cache dense** (i.e., [B,H,T,dh] like the standard model).
- Uses your FlashAttention kernel (flash_attn_triton_kvcache) for attention.
- **Avoids full [B,S,V] logits**: we call model.transformer(..., return_dict=True)
  and apply lm_head only on the last position for token selection.
- Separately measures prefill and decode memory/time:
    * prefill_ms, prefill_peak_MiB, prefill_end_alloc_MiB
    * decode_ms, decode_peak_MiB, decode_poststep_peak_MiB, decode_end_alloc_MiB
    * kv_end_MiB (allocator-accurate)
    * toks_per_s
- Tiny KV profiler shows per-phase QKV/attn/update times and KV read/write MiB.

Usage example
-------------
CUDA_VISIBLE_DEVICES=0 \
python3 profile_svd_kv_accum_flash.py \
  --decode-batch 16 \
  --prompt-len 256 \
  --decode-curve 128,256 \
  --rounds 1 \
  --rank-ratio-attn 1.0 \
  --rank-ratio-mlp 1.0 \
  --kv-profile
  
"""

import math, time, argparse, statistics, gc, os
from typing import Optional, Dict, Tuple, List

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import GPT2LMHeadModel, AutoTokenizer

from kernels.flash_attn_causal import flash_attn_triton_kvcache

MiB = float(1024**2)

# -------------------------
# Utils
# -------------------------
def set_seed(seed: int = 0):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def svd_factor(W: torch.Tensor, rank: int) -> Tuple[torch.Tensor, torch.Tensor]:
    """Return U_r, V_r such that W ≈ U_r @ V_r."""
    if W.dtype not in (torch.float32, torch.float64):
        W = W.float()
    W = W.contiguous()
    try:
        U, S, Vh = torch.linalg.svd(W, full_matrices=False)
    except TypeError:
        U_, S_, V_ = torch.svd(W)
        U, S, Vh = U_, S_, V_.t()
    r = min(rank, S.numel())
    U_r = U[:, :r].contiguous()
    V_r = (S[:r, None] * Vh[:r, :]).contiguous()
    return U_r, V_r

def as_linear_weight(W_raw: torch.Tensor, in_dim: int, out_dim: int) -> torch.Tensor:
    """Return weight in shape (in_dim, out_dim)."""
    if W_raw.shape == (in_dim, out_dim):
        return W_raw.contiguous()
    if W_raw.shape == (out_dim, in_dim):
        return W_raw.t().contiguous()
    raise ValueError(f"Unexpected weight shape {tuple(W_raw.shape)}; expected ({in_dim},{out_dim}) or ({out_dim},{in_dim}).")

# -------------------------
# KV Profiler (time/bytes/mem)
# -------------------------
class KVProfiler:
    """
    Per-phase profiler: 'prefill' and 'decode'.
    Tracks wall times, KV bytes, and memory checkpoints:
      - qkv_s, attn_s, update_s, kv_new_bytes, kv_read_bytes, calls
      - poststep_peak_bytes: max(memory_allocated) right after step
      - end_alloc_bytes: memory_allocated at end of phase
    """
    def __init__(self):
        self.enabled = False
        self.phase = "decode"
        self.reset()

    def reset(self):
        def zero():
            return dict(
                qkv_s=0.0, attn_s=0.0, update_s=0.0,
                kv_new_bytes=0, kv_read_bytes=0, calls=0,
                poststep_peak_bytes=0, end_alloc_bytes=0
            )
        self.stats = {"prefill": zero(), "decode": zero()}

    def enable(self, flag: bool = True):
        self.enabled = bool(flag)

    def set_phase(self, phase: str):
        self.phase = "prefill" if phase == "prefill" else "decode"

    def add_time(self, key: str, seconds: float):
        if self.enabled:
            self.stats[self.phase][key] += float(seconds)

    def add_bytes(self, key: str, nbytes: int):
        if self.enabled:
            self.stats[self.phase][key] += int(nbytes)

    def inc_calls(self):
        if self.enabled:
            self.stats[self.phase]["calls"] += 1

    def add_mem_poststep(self, bytes_now: int):
        if self.enabled:
            d = self.stats[self.phase]
            if bytes_now > d["poststep_peak_bytes"]:
                d["poststep_peak_bytes"] = bytes_now

    def set_end_alloc(self, bytes_now: int):
        if self.enabled:
            self.stats[self.phase]["end_alloc_bytes"] = bytes_now

    def snapshot(self) -> Dict[str, Dict[str, float]]:
        out = {}
        for ph, d in self.stats.items():
            out[ph] = {
                "qkv_ms": d["qkv_s"] * 1000.0,
                "attn_ms": d["attn_s"] * 1000.0,
                "update_ms": d["update_s"] * 1000.0,
                "kv_new_MiB": d["kv_new_bytes"] / MiB,
                "kv_read_MiB": d["kv_read_bytes"] / MiB,
                "poststep_peak_MiB": d["poststep_peak_bytes"] / MiB,
                "end_alloc_MiB": d["end_alloc_bytes"] / MiB,
                "calls": d["calls"],
            }
        return out

def attach_profiler(model: GPT2LMHeadModel, prof: Optional[KVProfiler]):
    if not hasattr(model, "transformer"):
        return
    for layer in model.transformer.h:
        if hasattr(layer, "profiler"):
            layer.profiler = prof
        blk = getattr(layer, "block", None)
        if blk is not None and hasattr(blk, "profiler"):
            blk.profiler = prof

# -------------------------
# SVD block (weights low-rank) + dense KV cache
# -------------------------
class LowRankSVDBlock(nn.Module):
    """
    GPT-2 block with per-head low-rank factors U,V for Q/K/V and MLP/out-proj.
    KV cache remains dense: we materialize dense K,V and return ONLY the new-step K,V.
    """
    def __init__(self, hf_layer: nn.Module, rank_ratio_attn: float = 1.0, rank_ratio_mlp: float = 1.0):
        super().__init__()
        attn = hf_layer.attn
        self.hf_attn = attn
        self.ln1 = hf_layer.ln_1
        self.ln2 = hf_layer.ln_2

        D = attn.embed_dim
        H = attn.num_heads
        if D % H != 0:
            raise ValueError(f"[LowRankSVDBlock] embed_dim={D} not divisible by heads={H}")
        dh = D // H

        self.D, self.H, self.dh = D, H, dh
        dev = next(hf_layer.parameters()).device
        ptdtype = next(hf_layer.parameters()).dtype

        # --- SVD for Q/K/V ---
        Wc = as_linear_weight(attn.c_attn.weight.data, in_dim=D, out_dim=3*D)
        bc = attn.c_attn.bias.data.clone().to(device=dev, dtype=ptdtype)
        q_w = Wc[:, :D].contiguous().view(D, H, dh)
        k_w = Wc[:, D:2*D].contiguous().view(D, H, dh)
        v_w = Wc[:, 2*D:3*D].contiguous().view(D, H, dh)
        q_b = bc[:D].view(H, dh).contiguous()
        k_b = bc[D:2*D].view(H, dh).contiguous()
        v_b = bc[2*D:3*D].view(H, dh).contiguous()

        r_attn = max(1, int(rank_ratio_attn * min(D, dh)))
        def alloc_uv(name: str):
            U = nn.Parameter(torch.empty(D, H, r_attn, device=dev, dtype=ptdtype))
            V = nn.Parameter(torch.empty(H, r_attn, dh, device=dev, dtype=ptdtype))
            self.register_parameter(f"{name}_U", U); self.register_parameter(f"{name}_V", V)
            return U, V
        self.q_U, self.q_V = alloc_uv("q")
        self.k_U, self.k_V = alloc_uv("k")
        self.v_U, self.v_V = alloc_uv("v")
        self.q_b = nn.Parameter(q_b.to(device=dev, dtype=ptdtype))
        self.k_b = nn.Parameter(k_b.to(device=dev, dtype=ptdtype))
        self.v_b = nn.Parameter(v_b.to(device=dev, dtype=ptdtype))

        with torch.no_grad():
            for name, W_h in (("q", q_w), ("k", k_w), ("v", v_w)):
                U_param = getattr(self, f"{name}_U"); V_param = getattr(self, f"{name}_V")
                Us, Vs = [], []
                for h in range(H):
                    Uh, Vh = svd_factor(W_h[:, h, :], r_attn)
                    Us.append(Uh.to(device=dev, dtype=ptdtype))
                    Vs.append(Vh.to(device=dev, dtype=ptdtype))
                U = torch.stack(Us, dim=1); V = torch.stack(Vs, dim=0)
                U_param.copy_(U); V_param.copy_(V)

        # --- SVD for out-proj & MLP ---
        W_out = as_linear_weight(attn.c_proj.weight.data, in_dim=D, out_dim=D)
        b_out = attn.c_proj.bias.data.clone().to(device=dev, dtype=ptdtype)
        r_out = max(1, int(rank_ratio_attn * min(W_out.shape)))
        Uo, Vo = svd_factor(W_out, r_out)
        self.out_U = nn.Parameter(Uo.to(device=dev, dtype=ptdtype))
        self.out_V = nn.Parameter(Vo.to(device=dev, dtype=ptdtype))
        self.out_b = nn.Parameter(b_out)

        I = hf_layer.mlp.c_fc.bias.data.numel()
        W1 = as_linear_weight(hf_layer.mlp.c_fc.weight.data, in_dim=D, out_dim=I)
        b1 = hf_layer.mlp.c_fc.bias.data.clone().to(device=dev, dtype=ptdtype)
        r_fc1 = max(1, int(rank_ratio_mlp * min(W1.shape)))
        U1, V1 = svd_factor(W1, r_fc1)
        self.fc1_U = nn.Parameter(U1.to(device=dev, dtype=ptdtype))
        self.fc1_V = nn.Parameter(V1.to(device=dev, dtype=ptdtype))
        self.fc1_b = nn.Parameter(b1)

        W2 = as_linear_weight(hf_layer.mlp.c_proj.weight.data, in_dim=I, out_dim=D)
        b2 = hf_layer.mlp.c_proj.bias.data.clone().to(device=dev, dtype=ptdtype)
        r_fc2 = max(1, int(rank_ratio_mlp * min(W2.shape)))
        U2, V2 = svd_factor(W2, r_fc2)
        self.fc2_U = nn.Parameter(U2.to(device=dev, dtype=ptdtype))
        self.fc2_V = nn.Parameter(V2.to(device=dev, dtype=ptdtype))
        self.fc2_b = nn.Parameter(b2)

        self.r_attn = r_attn
        self.profiler: Optional[KVProfiler] = None

    def forward(
        self,
        hidden_states: torch.Tensor,
        layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # [B,H,T_past,dh]
        attention_mask: Optional[torch.Tensor] = None,
        use_cache: bool = False,
        output_attentions: bool = False,
        **kwargs,
    ):
        B, S, D = hidden_states.shape
        dev = hidden_states.device
        H, dh, r = self.H, self.dh, self.r_attn
        prof: Optional[KVProfiler] = self.profiler

        x = self.ln1(hidden_states)

        # Low-rank Q,K,V then dense new-step K,V
        t0 = time.perf_counter()
        Q = torch.einsum('bsd,dhr,hre->bhse', x, self.q_U, self.q_V) + self.q_b[None, :, None, :]
        K_new = torch.einsum('bsd,dhr,hre->bhse', x, self.k_U, self.k_V) + self.k_b[None, :, None, :]
        V_new = torch.einsum('bsd,dhr,hre->bhse', x, self.v_U, self.v_V) + self.v_b[None, :, None, :]
        if prof: prof.add_time("qkv_s", time.perf_counter() - t0)

        # Concat with past (dense KV)
        if isinstance(layer_past, (tuple, list)) and len(layer_past) == 2 and layer_past[0] is not None:
            past_k, past_v = layer_past
            if past_k.dtype != K_new.dtype: past_k = past_k.to(dtype=K_new.dtype)
            if past_v.dtype != V_new.dtype: past_v = past_v.to(dtype=V_new.dtype)
            if past_k.device != K_new.device: past_k = past_k.to(K_new.device)
            if past_v.device != V_new.device: past_v = past_v.to(V_new.device)
            K_cat = torch.cat([past_k, K_new], dim=2)
            V_cat = torch.cat([past_v, V_new], dim=2)
        else:
            K_cat, V_cat = K_new, V_new

        # Build compact mask [B,H,1,S]
        if attention_mask is not None:
            if attention_mask.dim() == 4:
                q_mask = attention_mask[..., -S:].to(dtype=torch.bool)
                if q_mask.size(2) != 1:
                    q_mask = q_mask[..., :1, :]
            elif attention_mask.dim() == 2:
                q_mask = attention_mask[:, -S:].bool()[:, None, None, :]
            else:
                q_mask = torch.ones(B, 1, 1, S, dtype=torch.bool, device=dev)
        else:
            q_mask = torch.ones(B, 1, 1, S, dtype=torch.bool, device=dev)
        attn_mask_bh1s = q_mask.expand(B, H, 1, S).contiguous()

        # FlashAttention
        tA0 = time.perf_counter()
        Y_heads = flash_attn_triton_kvcache(Q, K_cat, V_cat, attn_mask_bh1s)  # [B,H,S,dh]
        if prof:
            prof.add_time("attn_s", time.perf_counter() - tA0)
            prof.add_bytes("kv_new_bytes", (K_new.numel() + V_new.numel()) * K_new.element_size())
            prof.add_bytes("kv_read_bytes", (K_cat.numel() + V_cat.numel()) * K_cat.element_size())
            prof.inc_calls()

        del Q, K_cat, V_cat, attn_mask_bh1s

        # Merge heads
        Y = Y_heads.transpose(1, 2).contiguous().view(B, S, self.D); del Y_heads

        # Out-proj + residual
        Y = torch.matmul(torch.matmul(Y, self.out_U), self.out_V); Y.add_(self.out_b)
        hidden_states = hidden_states.add(Y); del Y

        # MLP + residual
        z = self.ln2(hidden_states)
        t1 = torch.matmul(z, self.fc1_U); del z
        h1 = torch.matmul(t1, self.fc1_V); del t1
        h1.add_(self.fc1_b); h1 = F.gelu(h1)
        t2 = torch.matmul(h1, self.fc2_U); del h1
        h2 = torch.matmul(t2, self.fc2_V); del t2
        h2.add_(self.fc2_b)
        hidden_states.add_(h2); del h2

        outputs = (hidden_states,)
        if use_cache:
            outputs += ((K_new, V_new),)
        else:
            del K_new, V_new
        if output_attentions: outputs += (None,)
        return outputs

# -------------------------
# Shim: update dense KV cache in HF pipeline
# -------------------------
class LayerShim(nn.Module):
    def __init__(self, block: LowRankSVDBlock, layer_idx: int):
        super().__init__()
        self.block = block
        self.layer_idx = layer_idx
        self.profiler: Optional[KVProfiler] = None

    def forward(self, hidden_states, past_key_value=None, cache_position=None, attention_mask=None, *args, **kwargs):
        # Extract layer_past if present (assume [B,H,T,dh] if provided)
        layer_past = None
        expect_bthd = False  # if the cache happens to be [B,T,H,dh], we convert back later

        if past_key_value is not None and hasattr(past_key_value, "layers") and len(past_key_value.layers) > self.layer_idx:
            layer_cache = past_key_value.layers[self.layer_idx]
            k_cache = getattr(layer_cache, "keys", None)
            v_cache = getattr(layer_cache, "values", None)
            if k_cache is not None and v_cache is not None and k_cache.dim() == 4:
                expect_bthd = (k_cache.size(2) == self.block.H)  # True => [B,T,H,dh]
                if expect_bthd:
                    k_cache = k_cache.permute(0,2,1,3).contiguous()
                    v_cache = v_cache.permute(0,2,1,3).contiguous()
                layer_past = (k_cache, v_cache)

        # Forward through SVD block
        result = self.block(hidden_states, layer_past=layer_past, attention_mask=attention_mask,
                            use_cache=kwargs.get("use_cache", False),
                            output_attentions=kwargs.get("output_attentions", False))

        # If we produced new-step KV and have a cache object, update it
        if (past_key_value is not None and hasattr(past_key_value, "update") and
            isinstance(result, tuple) and len(result) >= 2 and isinstance(result[1], tuple) and len(result[1]) == 2):
            k_new, v_new = result[1]  # [B,H,S_new,dh]
            if expect_bthd:
                k_new = k_new.permute(0,2,1,3).contiguous()
                v_new = v_new.permute(0,2,1,3).contiguous()
            t0 = time.perf_counter()
            past_key_value.update(k_new, v_new, self.layer_idx)
            if self.profiler:
                self.profiler.add_time("update_s", time.perf_counter() - t0)

        return result

def build_svd_model(rank_ratio_attn: float, rank_ratio_mlp: float, device: Optional[str] = None) -> GPT2LMHeadModel:
    model = GPT2LMHeadModel.from_pretrained("gpt2")
    model.eval()
    if device: model = model.to(device)
    for p in model.parameters(): p.requires_grad = False
    for i, layer in enumerate(model.transformer.h):
        blk = LowRankSVDBlock(layer, rank_ratio_attn=rank_ratio_attn, rank_ratio_mlp=rank_ratio_mlp)
        shim = LayerShim(blk, layer_idx=i).to(device if device is not None else next(model.parameters()).device)
        model.transformer.h[i] = shim
    return model

# -------------------------
# KV bytes (allocator-accurate, dedup storages)
# -------------------------
@torch.no_grad()
def measure_kv_cache_bytes(past_key_values) -> int:
    """Walk HF cache object and sum underlying key/value tensor storages."""
    if past_key_values is None: return 0
    seen = set(); total = 0
    def rec(x):
        nonlocal total
        if torch.is_tensor(x) and x.is_cuda:
            try:
                s = x.untyped_storage(); key = (s.data_ptr(), int(s.nbytes()))
            except Exception:
                s = x.storage()
                key = (s.data_ptr() if hasattr(s, "data_ptr") else x.data_ptr(),
                       int(s.nbytes() if hasattr(s, "nbytes") else s.size()*x.element_size()))
            if key not in seen:
                seen.add(key); total += key[1]
        elif isinstance(x, (list, tuple, set)):
            for y in x: rec(y)
        elif isinstance(x, dict):
            for y in x.values(): rec(y)
        else:
            for name in ("layers","keys","values","key_cache","value_cache","k_cache","v_cache"):
                if hasattr(x, name): rec(getattr(x, name))
    rec(past_key_values); return total

# -------------------------
# Token picker: last position only (no full logits)
# -------------------------
@torch.no_grad()
def _next_from_last_hidden(model: GPT2LMHeadModel, last_hidden_state: torch.Tensor, greedy: bool = True) -> torch.Tensor:
    last = last_hidden_state[:, -1, :]     # [B,D]
    logits_last = model.lm_head(last)      # [B,V]
    if greedy: return logits_last.argmax(dim=-1, keepdim=True)
    probs = F.softmax(logits_last.float(), dim=-1); return torch.multinomial(probs, 1)

# -------------------------
# Prefill+Decode benchmark (clean split, no big logits)
# -------------------------
@torch.no_grad()
def decode_benchmark_svd(model: GPT2LMHeadModel, prompt: torch.Tensor, new_tokens: int, device: str,
                         profiler: Optional[KVProfiler] = None, greedy: bool = True) -> Dict[str, float]:
    model.eval(); B = prompt.size(0)
    attach_profiler(model, profiler)
    if profiler: profiler.reset(); profiler.enable(True)

    # ---- Prefill ----
    if torch.cuda.is_available():
        torch.cuda.empty_cache(); torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()
    if profiler: profiler.set_phase("prefill")

    t0 = time.perf_counter()
    out = model.transformer(input_ids=prompt, use_cache=True, return_dict=True)
    if torch.cuda.is_available(): torch.cuda.synchronize()
    prefill_ms = (time.perf_counter() - t0) * 1000.0

    next_id = _next_from_last_hidden(model, out.last_hidden_state, greedy=greedy)
    past = out.past_key_values  # HF cache object

    prefill_peak = torch.cuda.max_memory_allocated() / MiB if torch.cuda.is_available() else 0.0
    prefill_end_alloc = torch.cuda.memory_allocated() / MiB if torch.cuda.is_available() else 0.0
    if profiler: profiler.add_mem_poststep(int(prefill_end_alloc * MiB)); profiler.set_end_alloc(int(prefill_end_alloc * MiB))

    # Free prefill outputs before measuring decode
    del out; gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache(); torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()

    # ---- Decode ----
    if profiler: profiler.set_phase("decode")
    decode_poststep_peak = 0.0; t_dec = 0.0

    for _ in range(new_tokens):
        t1 = time.perf_counter()
        step = model.transformer(input_ids=next_id, past_key_values=past, use_cache=True, return_dict=True)
        if torch.cuda.is_available(): torch.cuda.synchronize()
        t_dec += (time.perf_counter() - t1)

        next_id = _next_from_last_hidden(model, step.last_hidden_state, greedy=greedy)
        past = step.past_key_values
        del step; gc.collect()

        if torch.cuda.is_available():
            alloc_now = torch.cuda.memory_allocated() / MiB
            if profiler: profiler.add_mem_poststep(int(alloc_now * MiB))
            if alloc_now > decode_poststep_peak: decode_poststep_peak = alloc_now

    decode_ms = t_dec * 1000.0
    decode_peak = torch.cuda.max_memory_allocated() / MiB if torch.cuda.is_available() else 0.0
    decode_end_alloc = torch.cuda.memory_allocated() / MiB if torch.cuda.is_available() else 0.0
    kv_end_mib = measure_kv_cache_bytes(past) / MiB
    if profiler: profiler.set_end_alloc(int(decode_end_alloc * MiB))

    toks_per_s = (B * max(new_tokens,1)) / max(t_dec,1e-6)
    return {
        "prefill_ms": prefill_ms, "decode_ms": decode_ms,
        "prefill_peak_MiB": prefill_peak, "prefill_end_alloc_MiB": prefill_end_alloc,
        "decode_peak_MiB": decode_peak, "decode_poststep_peak_MiB": decode_poststep_peak,
        "decode_end_alloc_MiB": decode_end_alloc,
        "kv_end_MiB": kv_end_mib, "toks_per_s": toks_per_s,
        "prof_snapshot": (profiler.snapshot() if profiler and profiler.enabled else None),
    }

def _fmt_mean_std(vals: List[float], width: int = None, prec: int = 2) -> str:
    if not vals: s = "nan"
    else:
        m = statistics.mean(vals); sd = statistics.pstdev(vals) if len(vals) >= 2 else 0.0
        s = f"{m:.{prec}f}±{sd:.{prec}f}"
    return f"{s:>{width}}" if width else s

@torch.no_grad()
def decode_growth_curve_svd(model: GPT2LMHeadModel, tokenizer: AutoTokenizer, device: str,
                            batch_size: int, prompt_len: int, curve_lens: List[int],
                            rounds: int = 3, kv_profile: bool = True):
    print(f"\n=== Decoding-time KV-cache growth (SVD weights + dense KV, last-token logits) — {rounds} rounds avg ===")
    vocab = tokenizer.vocab_size if hasattr(tokenizer, "vocab_size") else 50257
    prompt = torch.randint(0, min(1000, vocab), (batch_size, prompt_len), device=device)

    header = (f"{'new_T':>7} | {'t/s':>10} | {'prefill ms':>11} | {'decode ms':>10} | "
              f"{'prefill peak':>12} | {'dec peak':>9} | {'poststep':>9} | {'end_alloc':>9} | {'KV_end':>7}")
    print(header); print("-" * len(header))

    for idx, new_T in enumerate(curve_lens):
        tps, pre_ms, dec_ms, pre_peak, dec_peak, poststep, end_alloc, kv_end = [], [], [], [], [], [], [], []

        for r in range(rounds):
            if torch.cuda.is_available():
                torch.cuda.empty_cache(); torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()
            prof = KVProfiler() if kv_profile else None
            res = decode_benchmark_svd(model, prompt, new_T, device, profiler=prof, greedy=True)
            tps.append(res["toks_per_s"]); pre_ms.append(res["prefill_ms"]); dec_ms.append(res["decode_ms"])
            pre_peak.append(res["prefill_peak_MiB"]); dec_peak.append(res["decode_peak_MiB"])
            poststep.append(res["decode_poststep_peak_MiB"]); end_alloc.append(res["decode_end_alloc_MiB"])
            kv_end.append(res["kv_end_MiB"])

            if kv_profile and idx == 0 and r == 0 and res["prof_snapshot"]:
                snap = res["prof_snapshot"]
                print("\n  [KV Profiler — per-phase, aggregated across layers, round 1]")
                for ph in ("prefill","decode"):
                    s = snap[ph]; calls = int(s["calls"])
                    print(f"   {ph:>7}: qkv={s['qkv_ms']:7.1f}ms  attn={s['attn_ms']:7.1f}ms  "
                          f"upd={s['update_ms']:7.1f}ms  calls={calls:4d}  "
                          f"kv_new={s['kv_new_MiB']:7.1f}MiB  kv_read={s['kv_read_MiB']:7.1f}MiB  "
                          f"poststep_peak={s['poststep_peak_MiB']:7.1f}MiB  end_alloc={s['end_alloc_MiB']:7.1f}MiB")

        print(f"{new_T:7d} | {_fmt_mean_std(tps,10,2)} | {_fmt_mean_std(pre_ms,11,1)} | {_fmt_mean_std(dec_ms,10,1)} | "
              f"{_fmt_mean_std(pre_peak,12,1)} | {_fmt_mean_std(dec_peak,9,1)} | "
              f"{_fmt_mean_std(poststep,9,1)} | {_fmt_mean_std(end_alloc,9,1)} | {_fmt_mean_std(kv_end,7,1)}")

# -------------------------
# Main
# -------------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--rank-ratio-attn", type=float, default=1.0)
    parser.add_argument("--rank-ratio-mlp",  type=float, default=1.0)
    parser.add_argument("--seed", type=int, default=0)
    parser.add_argument("--decode-curve", type=str, default="64,128,256,512")
    parser.add_argument("--decode-batch", type=int, default=1)
    parser.add_argument("--prompt-len", type=int, default=32)
    parser.add_argument("--rounds", type=int, default=3)
    parser.add_argument("--kv-profile", action="store_true")
    args = parser.parse_args()

    set_seed(args.seed)
    device = "cuda" if torch.cuda.is_available() else "cpu"

    print("\n=== Building SVD-weights Model (dense KV) ===")
    model = build_svd_model(rank_ratio_attn=args.rank_ratio_attn,
                            rank_ratio_mlp=args.rank_ratio_mlp,
                            device=device)
    blk0 = model.transformer.h[0].block
    print(f"QKV rank={blk0.r_attn}, embed_dim={blk0.D}, heads={blk0.H}, dh={blk0.dh}")

    tok = AutoTokenizer.from_pretrained("gpt2")
    if tok.pad_token_id is None and tok.eos_token_id is not None:
        tok.pad_token = tok.eos_token

    curve = [int(x) for x in args.decode_curve.split(",") if x.strip()]
    bsz = args.decode_batch; p_len = args.prompt_len

    if torch.cuda.is_available():
        torch.cuda.empty_cache(); torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()

    decode_growth_curve_svd(model, tok, device=device,
                            batch_size=bsz, prompt_len=p_len,
                            curve_lens=curve, rounds=args.rounds,
                            kv_profile=args.kv_profile)

if __name__ == "__main__":
    main()
