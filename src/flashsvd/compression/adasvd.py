"""
Adaptive Rank Selection SVD (AdaSVD) compression for BERT and RoBERTa models.

Uses per-operation ranks from ranks.json generated by adaptive_rank_selection.py.
Extracts and adapts the proven AdaSVD logic from existing experiments.
"""

import json
import torch
import torch.nn as nn
from typing import Dict, Callable

# Import from flashsvd package (unified import path)
from flashsvd.utils import FlashSVDBlocks, svd_helpers


def _get_full_name(module: nn.Module) -> str:
    """Get the _ars_fullname attribute set during attach_fullnames."""
    return getattr(module, "_ars_fullname", "")


def _validate_ranks_json(ranks: Dict[str, int], arch: str) -> None:
    """
    Validate that ranks.json keys match the architecture.

    Args:
        ranks: Loaded ranks dictionary
        arch: Architecture name (bert or roberta)

    Raises:
        ValueError: If keys don't match architecture
    """
    if not ranks:
        raise ValueError("ranks.json is empty")

    # Check prefix of first key
    first_key = next(iter(ranks.keys()))

    if arch == "bert":
        if not first_key.startswith("bert."):
            raise ValueError(
                f"ranks.json contains keys starting with '{first_key.split('.')[0]}.*' "
                f"but architecture is 'bert'. Expected keys like 'bert.encoder.layer.0...'"
            )
    elif arch == "roberta":
        if not first_key.startswith("roberta."):
            raise ValueError(
                f"ranks.json contains keys starting with '{first_key.split('.')[0]}.*' "
                f"but architecture is 'roberta'. Expected keys like 'roberta.encoder.layer.0...'"
            )


def _extract_actual_ranks_bert(model: nn.Module) -> Dict[str, int]:
    """
    Extract actual ranks from AdaSVD-compressed BERT model.

    M7 Phase 2.3: After compression, ranks may differ from input ranks.json
    due to per-head clamping. This function reads actual parameter shapes.

    Args:
        model: Compressed BERT model with AdaSVD blocks

    Returns:
        Dict mapping layer names to actual ranks used
    """
    actual_ranks = {}

    for i, layer in enumerate(model.bert.encoder.layer):
        # Access inner block (unwrap BertLayerShim)
        block = layer.block if hasattr(layer, 'block') else layer

        # Extract ranks from parameter shapes
        # Attention: Pq shape = [H, d_model, rank_q]
        actual_ranks[f"bert.encoder.layer.{i}.attention.self.query"] = block.Pq.shape[2]
        actual_ranks[f"bert.encoder.layer.{i}.attention.self.key"] = block.Pk.shape[2]
        actual_ranks[f"bert.encoder.layer.{i}.attention.self.value"] = block.Pv.shape[2]
        actual_ranks[f"bert.encoder.layer.{i}.attention.output.dense"] = block.Uo.shape[1]

        # FFN: U1 shape = [d_model, rank_ffn1]
        actual_ranks[f"bert.encoder.layer.{i}.intermediate.dense"] = block.U1.shape[1]
        actual_ranks[f"bert.encoder.layer.{i}.output.dense"] = block.U2.shape[1]

    # Pooler (if exists)
    if hasattr(model, 'bert') and hasattr(model.bert, 'pooler') and model.bert.pooler is not None:
        if hasattr(model.bert.pooler.dense, 'weight'):
            # Pooler not compressed, use full rank
            actual_ranks["bert.pooler.dense"] = model.bert.pooler.dense.weight.shape[1]

    # Classifier (if exists)
    if hasattr(model, 'classifier'):
        # Classifier not compressed, use full rank
        actual_ranks["classifier"] = model.classifier.weight.shape[1]

    return actual_ranks


def compress_bert_adasvd(
    model: nn.Module,
    ranks: Dict[str, int],
    device: str = "cuda",
    ranks_dict: Dict[str, int] = None,
    ranks_json_path: str = None,
    ffn_kernel: str = "v1",
    build_only: bool = False,
    strict: bool = True,
    **kwargs
) -> tuple:
    """
    Apply Adaptive Rank Selection SVD compression to BERT model.

    M7 Phase 2.2: Supports direct ranks_dict passing (no temporary files).
    M7 Phase 2.3: Returns (model, actual_ranks) to fix metadata recording.

    Args:
        model: BERT model (e.g., BertForSequenceClassification)
        ranks: Summary ranks dict (attn/ffn/wo) for metadata only
        device: Device to perform compression on
        ranks_dict: Per-operation ranks dict (PRIORITY - from compression_info.json)
        ranks_json_path: Path to ranks.json file (fallback if ranks_dict not provided)
        ffn_kernel: FFN kernel variant ("v1" or "v2", default: "v1")
        build_only: If True, only create parameter shapes (M7 Phase 2.1)
        strict: If True, fail loudly if rank missing for any Linear

    Returns:
        (model, actual_ranks): Compressed model and dict of actual ranks used

        M7 Phase 2.3: actual_ranks contains post-clamping ranks (e.g., rank=64
        for attention Q/K/V even if ranks.json specified 497, due to per-head
        dimension constraints).

    Example:
        >>> # Direct dict passing (v2 loader)
        >>> model, actual_ranks = compress_bert_adasvd(
        ...     model, {}, "cuda",
        ...     ranks_dict={"bert.encoder.layer.0.attention.self.query": 497, ...},
        ...     build_only=True
        ... )
        >>> # File-based (CLI compression)
        >>> model, actual_ranks = compress_bert_adasvd(
        ...     model, {}, "cuda",
        ...     ranks_json_path="./BERTAda/ars_out/ranks.json"
        ... )
        >>> # actual_ranks["bert.encoder.layer.0.attention.self.query"] == 64 (clamped)
    """
    model = model.to(device).eval()

    # M7 Phase 2.2: Priority order: ranks_dict > ranks_json_path > error
    if ranks_dict is not None:
        # Direct dict provided (v2 loader path)
        print(f"Using provided per-operation ranks dict ({len(ranks_dict)} operations)")
        adasvd_ranks = ranks_dict
    elif ranks_json_path is not None:
        # Load from JSON file (CLI compression path)
        print(f"Loading AdaSVD ranks from: {ranks_json_path}")
        with open(ranks_json_path, "r") as f:
            adasvd_ranks = json.load(f)
    else:
        raise ValueError(
            "AdaSVD compression requires either:\n"
            "  1. ranks_dict (dict): Per-operation ranks dict, OR\n"
            "  2. ranks_json_path (str): Path to ranks.json file\n"
            "At least one must be provided."
        )

    # Validate ranks match BERT architecture
    _validate_ranks_json(adasvd_ranks, arch="bert")

    print(f"Loaded {len(adasvd_ranks)} per-operation ranks")

    # M7 Phase 2.1: Handle build_only mode
    if build_only:
        print("  [AdaSVD] build_only=True: SKIP SVD decomposition")

    # M7 Phase 2.2: Attach full names to all Linear modules (ALWAYS required for rank lookup)
    # Even in build_only mode, we need fullnames to map ranks_dict keys to modules
    svd_helpers.attach_fullnames(model)

    # Build SVD decomposition helpers (only if not build_only)
    if not build_only:
        svd_per_head, svd_low_rank = svd_helpers.build_plain_svd_helpers(model)
    else:
        # Dummy helpers (not used in build_only mode)
        svd_per_head, svd_low_rank = None, None

    # Replace each encoder layer with AdaSVD block
    print(f"Replacing layers with AdaSVD blocks (FFN kernel: {ffn_kernel}, build_only={build_only})...")
    for i, layer in enumerate(model.bert.encoder.layer):
        # Create AdaSVD block (performs per-op rank decomposition unless build_only)
        ada_block = FlashSVDBlocks.BertAdaSVDBlock(
            layer,
            ranks_dict=adasvd_ranks,  # M7 Phase 2.2: Use adasvd_ranks
            svd_per_head=svd_per_head,
            svd_low_rank=svd_low_rank,
            ffn_kernel=ffn_kernel,
            build_only=build_only,
        )

        # Wrap with LayerShim for HuggingFace compatibility
        shimmed_block = svd_helpers.BertLayerShim(ada_block)

        # Replace original layer
        model.bert.encoder.layer[i] = shimmed_block.to(device).eval()

    # Clean up helper functions to free memory
    if not build_only:
        del svd_per_head, svd_low_rank

    # Clean up cached helpers in layers
    for layer in model.bert.encoder.layer:
        if hasattr(layer, 'svd_per_head'):
            del layer.svd_per_head
        if hasattr(layer, 'svd_low_rank'):
            del layer.svd_low_rank

    torch.cuda.empty_cache()

    # M7 Phase 2.3: Extract actual ranks from compressed model
    if not build_only:
        actual_ranks = _extract_actual_ranks_bert(model)
        print(f"  Extracted {len(actual_ranks)} actual ranks after compression")
        # Debug: Show clamping examples
        for key in list(adasvd_ranks.keys())[:3]:
            if key in actual_ranks and key in adasvd_ranks:
                req = adasvd_ranks[key]
                act = actual_ranks[key]
                if req != act:
                    print(f"    Rank clamped: {key}: {req} → {act}")
    else:
        # In build_only mode, use provided ranks (already clamped in checkpoint)
        actual_ranks = adasvd_ranks

    return model, actual_ranks


def _extract_actual_ranks_roberta(model: nn.Module) -> Dict[str, int]:
    """
    Extract actual ranks from AdaSVD-compressed RoBERTa model.

    M7 Phase 2.3: After compression, ranks may differ from input ranks.json
    due to per-head clamping. This function reads actual parameter shapes.

    Args:
        model: Compressed RoBERTa model with AdaSVD blocks

    Returns:
        Dict mapping layer names to actual ranks used
    """
    actual_ranks = {}

    for i, layer in enumerate(model.roberta.encoder.layer):
        # Access inner block (unwrap BertLayerShim)
        block = layer.block if hasattr(layer, 'block') else layer

        # Extract ranks from parameter shapes
        # Attention: Pq shape = [H, d_model, rank_q]
        actual_ranks[f"roberta.encoder.layer.{i}.attention.self.query"] = block.Pq.shape[2]
        actual_ranks[f"roberta.encoder.layer.{i}.attention.self.key"] = block.Pk.shape[2]
        actual_ranks[f"roberta.encoder.layer.{i}.attention.self.value"] = block.Pv.shape[2]
        actual_ranks[f"roberta.encoder.layer.{i}.attention.output.dense"] = block.Uo.shape[1]

        # FFN: U1 shape = [d_model, rank_ffn1]
        actual_ranks[f"roberta.encoder.layer.{i}.intermediate.dense"] = block.U1.shape[1]
        actual_ranks[f"roberta.encoder.layer.{i}.output.dense"] = block.U2.shape[1]

    # Classifier (if exists)
    if hasattr(model, 'classifier'):
        # Classifier not compressed, use full rank
        actual_ranks["classifier"] = model.classifier.weight.shape[1]

    return actual_ranks


def compress_roberta_adasvd(
    model: nn.Module,
    ranks: Dict[str, int],
    device: str = "cuda",
    ranks_dict: Dict[str, int] = None,
    ranks_json_path: str = None,
    ffn_kernel: str = "v1",
    build_only: bool = False,
    strict: bool = True,
    **kwargs
) -> tuple:
    """
    Apply Adaptive Rank Selection SVD compression to RoBERTa model.

    M7 Phase 2.2: Supports direct ranks_dict passing (no temporary files).
    M7 Phase 2.3: Returns (model, actual_ranks) to fix metadata recording.

    Args:
        model: RoBERTa model (e.g., RobertaForSequenceClassification)
        ranks: Summary ranks dict (attn/ffn/wo) for metadata only
        device: Device to perform compression on
        ranks_dict: Per-operation ranks dict (PRIORITY - from compression_info.json)
        ranks_json_path: Path to ranks.json file (fallback if ranks_dict not provided)
        ffn_kernel: FFN kernel variant ("v1" or "v2", default: "v1")
        build_only: If True, only create parameter shapes (M7 Phase 2.1)
        strict: If True, fail loudly if rank missing for any Linear

    Returns:
        (model, actual_ranks): Compressed model and dict of actual ranks used

        M7 Phase 2.3: actual_ranks contains post-clamping ranks (e.g., rank=64
        for attention Q/K/V even if ranks.json specified 497, due to per-head
        dimension constraints).

    Example:
        >>> # Direct dict passing (v2 loader)
        >>> model, actual_ranks = compress_roberta_adasvd(
        ...     model, {}, "cuda",
        ...     ranks_dict={"roberta.encoder.layer.0.attention.self.query": 497, ...},
        ...     build_only=True
        ... )
    """
    model = model.to(device).eval()

    # M7 Phase 2.2: Priority order: ranks_dict > ranks_json_path > error
    if ranks_dict is not None:
        # Direct dict provided (v2 loader path)
        print(f"Using provided per-operation ranks dict ({len(ranks_dict)} operations)")
        adasvd_ranks = ranks_dict
    elif ranks_json_path is not None:
        # Load from JSON file (CLI compression path)
        print(f"Loading AdaSVD ranks from: {ranks_json_path}")
        with open(ranks_json_path, "r") as f:
            adasvd_ranks = json.load(f)
    else:
        raise ValueError(
            "AdaSVD compression requires either:\n"
            "  1. ranks_dict (dict): Per-operation ranks dict, OR\n"
            "  2. ranks_json_path (str): Path to ranks.json file\n"
            "At least one must be provided."
        )

    # Validate ranks match RoBERTa architecture
    _validate_ranks_json(adasvd_ranks, arch="roberta")

    print(f"Loaded {len(adasvd_ranks)} per-operation ranks")

    # M7 Phase 2.1: Handle build_only mode
    if build_only:
        print("  [RobertaAdaSVD] build_only=True: SKIP SVD decomposition")

    # M7 Phase 2.2: Attach full names to all Linear modules (ALWAYS required for rank lookup)
    # Even in build_only mode, we need fullnames to map ranks_dict keys to modules
    svd_helpers.attach_fullnames(model)

    # Build SVD decomposition helpers (only if not build_only)
    if not build_only:
        svd_per_head, svd_low_rank = svd_helpers.build_plain_svd_helpers(model)
    else:
        # Dummy helpers (not used in build_only mode)
        svd_per_head, svd_low_rank = None, None

    # Replace each encoder layer with AdaSVD block
    print(f"Replacing layers with AdaSVD blocks (FFN kernel: {ffn_kernel}, build_only={build_only})...")
    for i, layer in enumerate(model.roberta.encoder.layer):
        # Create AdaSVD block (performs per-op rank decomposition unless build_only)
        ada_block = FlashSVDBlocks.RobertaAdaSVDBlock(
            layer,
            ranks_dict=adasvd_ranks,  # M7 Phase 2.2: Use adasvd_ranks
            svd_per_head=svd_per_head,
            svd_low_rank=svd_low_rank,
            ffn_kernel=ffn_kernel,
            build_only=build_only,
        )

        # Wrap with LayerShim for HuggingFace compatibility
        # RoBERTa uses the same BertLayerShim as BERT
        shimmed_block = svd_helpers.BertLayerShim(ada_block)

        # Replace original layer
        model.roberta.encoder.layer[i] = shimmed_block.to(device).eval()

    # Clean up helper functions to free memory
    if not build_only:
        del svd_per_head, svd_low_rank

    # Clean up cached helpers in layers
    for layer in model.roberta.encoder.layer:
        if hasattr(layer, 'svd_per_head'):
            del layer.svd_per_head
        if hasattr(layer, 'svd_low_rank'):
            del layer.svd_low_rank

    torch.cuda.empty_cache()

    # M7 Phase 2.3: Extract actual ranks from compressed model
    if not build_only:
        actual_ranks = _extract_actual_ranks_roberta(model)
        print(f"  Extracted {len(actual_ranks)} actual ranks after compression")
        # Debug: Show clamping examples
        for key in list(adasvd_ranks.keys())[:3]:
            if key in actual_ranks and key in adasvd_ranks:
                req = adasvd_ranks[key]
                act = actual_ranks[key]
                if req != act:
                    print(f"    Rank clamped: {key}: {req} → {act}")
    else:
        # In build_only mode, use provided ranks (already clamped in checkpoint)
        actual_ranks = adasvd_ranks

    return model, actual_ranks
